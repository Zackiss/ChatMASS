[model]
layer_num = 12
head_num = 12
embedding_dim = 768

[dropout]
embedding = 0.1
attention = 0.1
resid = 0.1

[vocab]
block_size = 40
vocab_size = 40

[train]
device = 'auto'
num_dataloader = 4
max_iters = None
batch_size = 64
learning_rate = 3e-4
betas = (0.9, 0.95)
weight_decay = 0.1
grad_norm_clip = 1.0