[model]
layer_num = 15
head_num = 13
embedding_dim = 280
tokenization = True
token_batch = 10

[dropout]
embedding = 0.05
attention = 0.1
resid = 0.05

[vocab]
block_size = 128

[train]
device = auto
num_workers = 4
max_iters = 5000
batch_size = 64
learning_rate = 6e-3
weight_decay = 0.1
grad_norm_clip = 1.0
gradient_accumulation_steps = 5